{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating hybrid dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, a hybrid dataset is created from the byte file and ASM file. Byte file features are extracted using two models, one is a simple CNN and another one is CNN with pretrained encoder layers. All byte file data pass through both of these pretrained models for feature extraction and then concatenated with the ASM file feature which was selected using SVM classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries and initailizing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "from cnn_v1 import ConvNet_single\n",
    "import torch.nn.functional as F\n",
    "from dataloader_csv import CustomDatasetFromImages\n",
    "import csv\n",
    "from cnn_stacking import autoencoder,autoencoder1\n",
    "from cnn_stacking import Cnn_Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As pretrained models are store in Cuda version, so for loading them GPU must be available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available(): \n",
    "    print(\"gpu available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading byte File data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path=  'data_to_test.csv'\n",
    "\n",
    "test_data = CustomDatasetFromImages(test_path)\n",
    "\n",
    "test_data_loader  = data.DataLoader(test_data, batch_size=1, shuffle=False) \n",
    "\n",
    "train_path=  'data_to_tra.csv'\n",
    "\n",
    "train_data = CustomDatasetFromImages(train_path)\n",
    "\n",
    "train_data_loader  = data.DataLoader(train_data, batch_size=1, shuffle=False) \n",
    "\n",
    "valid_path=  'data_to_val.csv'\n",
    "\n",
    "valid_data = CustomDatasetFromImages(valid_path)\n",
    "\n",
    "valid_data_loader  = data.DataLoader(valid_data, batch_size=1, shuffle=False) \n",
    "\n",
    "fulL_train_path=  'data_to_traFull.csv'\n",
    "\n",
    "fulL_train_data = CustomDatasetFromImages(fulL_train_path)\n",
    "\n",
    "fulL_train_data_loader  = data.DataLoader(fulL_train_data, batch_size=1, shuffle=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the First model, which will extract features from byte files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= ConvNet_single().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the best model state during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_checkpoint = '..../saved_model_state/'\n",
    "\n",
    "scores=[]\n",
    "List= os.listdir(path_to_checkpoint)\n",
    "for i in range(len(List)):\n",
    "    scores.append(float(List[i].split()[0]))\n",
    "max_idx= np.argmin(scores)\n",
    "min_loss_path= path_to_checkpoint+List[max_idx]\n",
    "if (len(List)!=0):        \n",
    "    checkpoint= torch.load(min_loss_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    print(\"model initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the ASM file data(opcode frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tr='n_500_data_to_tra.csv'\n",
    "path_futr='n_500_data_to_traFull.csv'\n",
    "path_te='n_500_data_to_test.csv'\n",
    "path_val='n_500_data_to_val.csv'\n",
    "\n",
    "train = np.genfromtxt(path_tr, delimiter=\",\", dtype=str)\n",
    "\n",
    "val = np.genfromtxt(path_val, delimiter=\",\", dtype=str)\n",
    "\n",
    "test = np.genfromtxt(path_te, delimiter=\",\", dtype=str)\n",
    "\n",
    "full_train = np.genfromtxt(path_futr, delimiter=\",\", dtype=str)\n",
    "\n",
    "train=train[1:,:]\n",
    "\n",
    "val=val[1:,:]\n",
    "\n",
    "test=test[1:,:]\n",
    "\n",
    "full_train=full_train[1:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following piece of code extracts the feature from test data of byte file and concatenate them to top 116 features selected by SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_write_test=[]\n",
    "\n",
    "for image,name,label in test_data_loader:\n",
    "    image=image.to(device)\n",
    "\n",
    "    output=model(image)\n",
    "    output=F.softmax(output)\n",
    "    output = output.reshape(output.size(0), -1)\n",
    "    \n",
    "    output=output.to('cpu')\n",
    "    output=output.detach().numpy()\n",
    "    output=output.tolist()\n",
    "    output=output[0]\n",
    "    for i in test:\n",
    "        if i[-2]==name[0]:\n",
    "            print(i[-2],name[0])\n",
    "            for j in i[0:116]:\n",
    "                output.append(j)\n",
    "            output.append(i[-2])\n",
    "            output.append(i[-1])\n",
    "            \n",
    "    data_to_write_test.append(output)\n",
    "\n",
    "myFile = open('final_hybrid_test.csv', 'w', newline='')  \n",
    "\n",
    "with myFile:  \n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(data_to_write_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following piece of code extracts the feature from train data of byte file and concatenate them to top 116 features selected by SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_write_train=[]\n",
    "\n",
    "for image,name,label in train_data_loader:\n",
    "    image=image.to(device)\n",
    "    output=model(image)\n",
    "    output=F.softmax(output) \n",
    "    output = output.reshape(output.size(0), -1)\n",
    "    \n",
    "    output=output.to('cpu')\n",
    "    output=output.detach().numpy()\n",
    "    output=output.tolist()\n",
    "    output=output[0]\n",
    "    for i in train:\n",
    "        if i[-2]==name[0]:\n",
    "            print(i[-2],name[0])\n",
    "            for j in i[0:116]:\n",
    "                output.append(j)\n",
    "            output.append(i[-2])\n",
    "            output.append(i[-1])\n",
    "            \n",
    "    data_to_write_train.append(output)\n",
    "\n",
    "myFile = open('final_hybrid_train.csv', 'w', newline='')  \n",
    "\n",
    "with myFile:  \n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(data_to_write_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following piece of code extracts the feature from validation data of byte file and concatenate them to top 116 features selected by SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_write_val=[]\n",
    "\n",
    "for image,name,label in valid_data_loader:\n",
    "    image=image.to(device)\n",
    "    output=model(image)\n",
    "    output=F.softmax(output)\n",
    "    \n",
    "    output = output.reshape(output.size(0), -1)\n",
    "    \n",
    "    output=output.to('cpu')\n",
    "    output=output.detach().numpy()\n",
    "    output=output.tolist()\n",
    "    output=output[0]\n",
    "    for i in val:\n",
    "        if i[-2]==name[0]:\n",
    "            print(i[-2],name[0])\n",
    "            for j in i[0:116]:\n",
    "                output.append(j)\n",
    "            output.append(i[-2])\n",
    "            output.append(i[-1])\n",
    "            \n",
    "    data_to_write_val.append(output)\n",
    "myFile = open('final_hybrid_valid.csv', 'w', newline='')  \n",
    "\n",
    "with myFile:  \n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(data_to_write_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following piece of code extracts the feature from overall train data of byte file and concatenate them to top 116 features selected by SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_write_full_train=[]\n",
    "\n",
    "for image,name,label in fulL_train_data_loader:\n",
    "    image=image.to(device)\n",
    "    output=model(image)\n",
    "    output=F.softmax(output)\n",
    "    \n",
    "    output = output.reshape(output.size(0), -1)\n",
    "    \n",
    "    output=output.to('cpu')\n",
    "    output=output.detach().numpy()\n",
    "    output=output.tolist()\n",
    "    output=output[0]\n",
    "    for i in full_train:\n",
    "        if i[-2]==name[0]:\n",
    "            print(i[-2],name[0])\n",
    "            for j in i[0:116]:\n",
    "                output.append(j)\n",
    "            output.append(i[-2])\n",
    "            output.append(i[-1])\n",
    "            \n",
    "    data_to_write_full_train.append(output)\n",
    "\n",
    "myFile = open('final_hybrid_full_train.csv', 'w', newline='')  \n",
    "\n",
    "with myFile:  \n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(data_to_write_full_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the second model (CNN with pretrained encoder layers) from which feature of byte file will be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto=autoencoder().to(device).float()\n",
    "auto1=autoencoder1().to(device).float()\n",
    "\n",
    "model=Cnn_Stacking(auto,auto1).to(device).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the best model state during FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_checkpoint='...//cnn_auto_saved_model//'\n",
    "\n",
    "scores=[]\n",
    "List= os.listdir(path_to_checkpoint)\n",
    "for i in range(len(List)):\n",
    "    scores.append(float(List[i].split()[0]))\n",
    "max_idx= np.argmin(scores)\n",
    "min_loss_path= path_to_checkpoint+List[max_idx]\n",
    "if (len(List)!=0):        \n",
    "    checkpoint= torch.load(min_loss_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    print(\"model initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from above processed csv ( extracted features(byte file) + 116 features(asm) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tr='final_hybrid_train.csv'\n",
    "\n",
    "path_futr='final_hybrid_full_train.csv'\n",
    "\n",
    "path_te='final_hybrid_test.csv'\n",
    "\n",
    "path_val='final_hybrid_valid.csv'\n",
    "\n",
    "train = np.genfromtxt(path_tr, delimiter=\",\", dtype=str)\n",
    "\n",
    "val = np.genfromtxt(path_val, delimiter=\",\", dtype=str)\n",
    "\n",
    "test = np.genfromtxt(path_te, delimiter=\",\", dtype=str)\n",
    "\n",
    "full_train = np.genfromtxt(path_futr, delimiter=\",\", dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following piece of code extracts the feature from test data of byte file using fine tunned CNN and concatenate them to previous extracted and selected features to generate hybrid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_write_test=[]\n",
    "\n",
    "for image,name,label in test_data_loader:\n",
    "    image=image.to(device)\n",
    "    output=model(image)\n",
    "    output=F.softmax(output)\n",
    "    output = output.reshape(output.size(0), -1)\n",
    "    output=output.to('cpu')\n",
    "    output=output.detach().numpy()\n",
    "    output=output.tolist()\n",
    "    output=output[0]\n",
    "    for i in test:\n",
    "        if i[-2]==name[0]:\n",
    "            print(i[-2],name[0])\n",
    "            for j in i:\n",
    "                output.append(j)\n",
    "            \n",
    "    data_to_write_test.append(output)\n",
    "\n",
    "myFile = open('final_hybrid_test.csv', 'w', newline='')  \n",
    "\n",
    "with myFile:  \n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(data_to_write_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following piece of code extracts the feature from train data of byte file using fine tunned CNN and concatenate them to previous extracted and selected features to generate hybrid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_write_train=[]\n",
    "\n",
    "for image,name,label in train_data_loader:\n",
    "    image=image.to(device)\n",
    "    output=model(image)\n",
    "    output=F.softmax(output)\n",
    "    \n",
    "    output = output.reshape(output.size(0), -1)\n",
    "    \n",
    "    output=output.to('cpu')\n",
    "    output=output.detach().numpy()\n",
    "    output=output.tolist()\n",
    "    output=output[0]\n",
    "    for i in train:\n",
    "        if i[-2]==name[0]:\n",
    "            print(i[-2],name[0])\n",
    "            for j in i:\n",
    "                output.append(j)\n",
    "            \n",
    "    data_to_write_train.append(output)\n",
    "\n",
    "myFile = open('final_hybrid_train.csv', 'w', newline='')  \n",
    "\n",
    "with myFile:  \n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(data_to_write_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following piece of code extracts the feature from the validation data of byte file using fine tunned CNN and concatenate them to previous extracted and selected features to generate hybrid data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_write_val=[]\n",
    "\n",
    "for image,name,label in valid_data_loader:\n",
    "    image=image.to(device)\n",
    "    output=model(image)\n",
    "    output=F.softmax(output)\n",
    "    \n",
    "    output = output.reshape(output.size(0), -1)\n",
    "    \n",
    "    output=output.to('cpu')\n",
    "    output=output.detach().numpy()\n",
    "    output=output.tolist()\n",
    "    output=output[0]\n",
    "    for i in val:\n",
    "        if i[-2]==name[0]:\n",
    "            print(i[-2],name[0])\n",
    "            for j in i:\n",
    "                output.append(j)\n",
    "            \n",
    "    data_to_write_val.append(output)\n",
    "myFile = open('final_hybrid_valid.csv', 'w', newline='')  \n",
    "\n",
    "with myFile:  \n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(data_to_write_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following piece of code extracts the feature from overall data of byte file using fine tunned CNN and concatenate them to previous extracted and selected features to generate hybrid data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_write_full_train=[]\n",
    "\n",
    "for image,name,label in fulL_train_data_loader:\n",
    "    image=image.to(device)\n",
    "    output=model(image)\n",
    "    output=F.softmax(output)\n",
    "    \n",
    "    output = output.reshape(output.size(0), -1)\n",
    "    \n",
    "    output=output.to('cpu')\n",
    "    output=output.detach().numpy()\n",
    "    output=output.tolist()\n",
    "    output=output[0]\n",
    "    for i in full_train:\n",
    "        if i[-2]==name[0]:\n",
    "            print(i[-2],name[0])\n",
    "            for j in i:\n",
    "                output.append(j)\n",
    "\n",
    "    \n",
    "    data_to_write_full_train.append(output)\n",
    "    \n",
    "myFile = open('final_hybrid_full_train.csv', 'w', newline='')  \n",
    "\n",
    "with myFile:  \n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(data_to_write_full_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
